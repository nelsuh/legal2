import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from concurrent.futures import ThreadPoolExecutor

import json
import re
from time import sleep
from requests.exceptions import RequestException
from selenium.common.exceptions import WebDriverException

def setup_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

def fetch_texts(url, title, retry=3):
    driver = setup_driver()
    texts_mon = ""
    texts_eng = ""

    try:
        for attempt in range(retry):
            try:
                response = requests.get(url, timeout=10)
                if response.status_code == 200:
                    break
            except RequestException as e:
                if attempt < retry - 1:  # i.e., before the last attempt
                    sleep(2)  # Wait 2 seconds before retrying
                    continue
                else:
                    raise

        soup = BeautifulSoup(response.text, 'html.parser')

        # Fetch Mongolian text
        element_mon = soup.find('div', class_='maincontenter w-100 pull-left')
        if element_mon:
            texts_mon = element_mon.get_text(strip=True).replace("Хэвлэх", "")
            start_index = texts_mon.find('1 дүгээр зүйл.')
            if start_index != -1:
                texts_mon = texts_mon[start_index:]

        # Navigate to the page with Selenium
        driver.get(url)
        try:
            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'a[href="#active-tab-11"]')))
            element_eng = soup.find('div', class_='w-100 pull-left --nomigration')
            if element_eng:
                texts_eng = element_eng.get_text()
                start_index = texts_eng.find('Article 1.')
                if start_index != -1:
                    texts_eng = texts_eng[start_index:]
        except WebDriverException:
            texts_eng = "not available."

    except Exception as e:
        print(f"Error processing {url}: {str(e)}")
    finally:
        driver.quit()

    return {'title': title, 'mon': texts_mon, 'eng': texts_eng}

# all_links = ['https://legalinfo.mn/mn/detail?lawId=1', 'https://legalinfo.mn/mn/detail?lawId=8928', 'https://legalinfo.mn/mn/detail?lawId=28']
# all_titles= ['АВЛИГЫН ЭСРЭГ ХУУЛИЙН ЗАРИМ ЗААЛТЫГ ДАГАЖ МӨРДӨХ ЖУРМЫН ТУХАЙ', 'АВЛИГЫН ЭСРЭГ ХУУЛЬ', 'АВТОБЕНЗИН, ДИЗЕЛИЙН ТҮЛШНИЙ АЛБАН ТАТВАРЫН ТУХАЙ']

with ThreadPoolExecutor(max_workers=5) as executor:
    results = executor.map(fetch_texts, all_links, all_titles)

with open('last2.jsonl', 'w', encoding='utf-8') as f:
    for result in results:
        json_line = json.dumps(result, ensure_ascii=False)
        f.write(json_line + '\n')

print("Processing complete. Data written to output3.jsonl.")

